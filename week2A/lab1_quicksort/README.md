# Lab 1: Quicksort Performance Analysis

## Overview

This starter code provides a simple quicksort implementation that uses the **first element as pivot**. This classic choice leads to O(n²) worst-case behavior on sorted input.

Your task is to profile this implementation, collect data, and explain *why* sorted input is so much slower.

## Files

```
lab1_quicksort/
├── Makefile              # Build and test targets
├── README.md             # This file
├── main.c                # Driver program
├── src/
│   └── quicksort.c       # Quicksort implementation
├── datasets/             # Test inputs (generated by make datasets)
│   ├── random_10000.txt
│   ├── sorted_10000.txt
│   ├── reverse_10000.txt
│   └── nearly_sorted_10000.txt
└── outputs/              # Sorted output files (created on run)
```

## Quick Start

```bash
# 1. Generate datasets (if not present)
make datasets

# 2. Compile
make clean && make

# 3. Run on a single dataset
./qs datasets/random_10000.txt

# 4. Time a single run
time ./qs datasets/sorted_10000.txt

# 5. Run perf
perf stat ./qs datasets/random_10000.txt

# 6. Run all tests
make test

# 7. Run perf on all datasets
make perf-all
```

## The Key Insight

The quicksort implementation in `src/quicksort.c` uses the first element as pivot.

For random input:
- Pivots are roughly median → balanced partitions
- Recursion depth ≈ log n
- Total comparisons ≈ n log n

For sorted input:
- Pivot is always the minimum → maximally unbalanced partitions
- Partition produces (0, 1, n-1) split every time
- Recursion depth = n
- Total comparisons ≈ n²/2

## What to Measure

1. **User time** (from `time` command) — CPU time in your program
2. **Cycles and instructions** (from `perf stat`) — total work done
3. **IPC** (instructions per cycle) — efficiency
4. **Cache misses** — memory access patterns
5. **Branch misses** — prediction accuracy
6. **Peak memory** (from `valgrind massif`) — stack depth effect

## Expected Results

| Dataset | Relative Time |
|---------|---------------|
| random | 1x (baseline) |
| sorted | 50-100x slower |
| reverse | 50-100x slower |
| nearly_sorted | 10-30x slower |

Your numbers will vary based on hardware, but the pattern should be clear.

## Generating Different Sizes

To test different input sizes, modify the Makefile targets or use:

```bash
# Random 5000 elements
shuf -i 1-100000 -n 5000 > datasets/random_5000.txt

# Sorted 5000 elements
seq 1 5000 > datasets/sorted_5000.txt
```

## Troubleshooting

### "Permission denied" for perf
```bash
sudo perf stat ./qs datasets/random_10000.txt
# Or:
sudo sysctl kernel.perf_event_paranoid=1
```

### valgrind not found
```bash
sudo apt install valgrind
```

### Datasets not generated
```bash
make datasets
```

## Tips for Analysis

1. **Don't just report numbers** — explain what they mean
2. **Compare ratios** — "sorted is 75x slower than random"
3. **Look for patterns** — do cycles and time increase together?
4. **Think about mechanisms** — is this a cache issue? Branch issue? Algorithm issue?

The answer: it's primarily an **algorithmic** issue, not hardware. The code does more work (more comparisons, more recursive calls), not less efficient work.
